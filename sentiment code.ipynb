{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kkxM9hcRk6h",
        "outputId": "82f423bb-b19a-4b4c-ec15-0465fddc81b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-26 14:39:33--  https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/main/train_split.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 144474 (141K) [text/plain]\n",
            "Saving to: ‘train_split.csv’\n",
            "\n",
            "train_split.csv     100%[===================>] 141.09K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-09-26 14:39:33 (6.09 MB/s) - ‘train_split.csv’ saved [144474/144474]\n",
            "\n",
            "--2024-09-26 14:39:33--  https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/main/test_split.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35259 (34K) [text/plain]\n",
            "Saving to: ‘test_split.csv’\n",
            "\n",
            "test_split.csv      100%[===================>]  34.43K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2024-09-26 14:39:33 (4.58 MB/s) - ‘test_split.csv’ saved [35259/35259]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/main/train_split.csv\n",
        "!wget https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/main/test_split.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv('train_split.csv')\n",
        "test_df = pd.read_csv('test_split.csv')"
      ],
      "metadata": {
        "id": "KbLgIhxDRlnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSDvGDiKRz7_",
        "outputId": "2ef302bc-0894-4db2-9111-954c8347f242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')  # Optional: for more language support"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7C3GJzJSE-D",
        "outputId": "2d8bdbfc-626e-40b2-bac2-02c20ceb6f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "import re\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "dMZ09ei0SHpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ],
      "metadata": {
        "id": "62TtRwohSKIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_lexicon = {\n",
        "    'joy': ['buoyed','gorgeous','grand','breeze','delicious','home','excited', 'happy', 'giggle', 'awe','joy','celebrate','party','sun','shine','laugh', 'smile', 'satisfied', 'pleased','ecstatic','enjoyment','warm','melted','giggling','impressed','love','dancing','helping','roses','balloons','marriage','magic'],\n",
        "    'fear': ['trauma','spooky','nightmare','intruder','ghost','odd','blackout','shaking','invisible','trouble', 'caught', 'trapped', 'overwhelmed','freak', 'seep', 'vanish', 'slaughter', 'danger', 'worry','scared','frightened','terrified','panic','nervous','dark','trapped','unnerving','unconscious','shook','petrified','turbulence','intense','storm','creepy','scream','psycho','prayer','unaware','blood','terror','gruesome','pressure','threat','gun'],\n",
        "    'anger': ['drug','embarrassment','wtf','annoyed','mess','pissed','angry', 'furious', 'rage', 'mad', 'fucking', 'broke','hate','bitch','irritated','yell','snarled','disagreed','worst','spite','bullshit'],\n",
        "    'sadness': ['illness','guilt','confusion','pain','down','grief','trouble', 'sadness', 'tears', 'weary', 'sorrow', 'sad', 'broken', 'fell', 'mind', 'buried','cry','hurt','ill','suffering','disappointed','pain','heaviness','ache','lonely','heartbreaking','awful'],\n",
        "    'surprise': ['surreal','loose','suddenly','what','wonder','surprised', 'shocked', 'astonished', 'amazed', 'realized', 'gasp','unexpected', 'wow','mysterious','strange','freaky','weird','hallucinate']\n",
        "}\n",
        "\n",
        "def preprocess_text(text):\n",
        "    contractions = {\n",
        "        \"ain't\": \"am not\",\n",
        "        \"aren't\": \"are not\",\n",
        "        \"can't\": \"cannot\",\n",
        "        \"could've\": \"could have\",\n",
        "        \"couldn't\": \"could not\",\n",
        "        \"didn't\": \"did not\",\n",
        "        \"doesn't\": \"does not\",\n",
        "        \"don't\": \"do not\",\n",
        "        \"hadn't\": \"had not\",\n",
        "        \"hasn't\": \"has not\",\n",
        "        \"haven't\": \"have not\",\n",
        "        \"he'd\": \"he would\",\n",
        "        \"he'll\": \"he will\",\n",
        "        \"he's\": \"he is\",\n",
        "        \"how'd\": \"how did\",\n",
        "        \"how'll\": \"how will\",\n",
        "        \"how's\": \"how is\",\n",
        "        \"i'd\": \"i would\",\n",
        "        \"i'll\": \"i will\",\n",
        "        \"i'm\": \"i am\",\n",
        "        \"i've\": \"i have\",\n",
        "        \"isn't\": \"is not\",\n",
        "        \"it'd\": \"it would\",\n",
        "        \"it'll\": \"it will\",\n",
        "        \"it's\": \"it is\",\n",
        "        \"let's\": \"let us\",\n",
        "        \"might've\": \"might have\",\n",
        "        \"mightn't\": \"might not\",\n",
        "        \"must've\": \"must have\",\n",
        "        \"mustn't\": \"must not\",\n",
        "        \"she'd\": \"she would\",\n",
        "        \"she'll\": \"she will\",\n",
        "        \"she's\": \"she is\",\n",
        "        \"should've\": \"should have\",\n",
        "        \"shouldn't\": \"should not\",\n",
        "        \"that's\": \"that is\",\n",
        "        \"there's\": \"there is\",\n",
        "        \"they'd\": \"they would\",\n",
        "        \"they'll\": \"they will\",\n",
        "        \"they're\": \"they are\",\n",
        "        \"they've\": \"they have\",\n",
        "        \"we'd\": \"we would\",\n",
        "        \"we'll\": \"we will\",\n",
        "        \"we're\": \"we are\",\n",
        "        \"we've\": \"we have\",\n",
        "        \"weren't\": \"were not\",\n",
        "        \"what'll\": \"what will\",\n",
        "        \"what's\": \"what is\",\n",
        "        \"where's\": \"where is\",\n",
        "        \"who'd\": \"who would\",\n",
        "        \"who'll\": \"who will\",\n",
        "        \"who's\": \"who is\",\n",
        "        \"won't\": \"will not\",\n",
        "        \"would've\": \"would have\",\n",
        "        \"wouldn't\": \"would not\",\n",
        "        \"you'd\": \"you would\",\n",
        "        \"you'll\": \"you will\",\n",
        "        \"you're\": \"you are\",\n",
        "        \"you've\": \"you have\"\n",
        "    }\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    for contraction, expanded in contractions.items():\n",
        "        text = re.sub(r'\\b' + contraction + r'\\b', expanded, text)\n",
        "\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(pos)) for token, pos in pos_tags]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "def lemmatize_emotion_lexicon(lexicon, lemmatizer):\n",
        "    lemmatized_lexicon = {}\n",
        "    for emotion, words in lexicon.items():\n",
        "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "        lemmatized_lexicon[emotion] = lemmatized_words\n",
        "    return lemmatized_lexicon\n",
        "\n",
        "emotion_lexicon_lemmatized = lemmatize_emotion_lexicon(emotion_lexicon, lemmatizer)\n",
        "\n",
        "def extract_emotions(text):\n",
        "    tokens = word_tokenize(preprocess_text(text))\n",
        "    bigrams = [' '.join(bigram) for bigram in ngrams(tokens, 2)]\n",
        "    trigrams = [' '.join(trigram) for trigram in ngrams(tokens, 3)]\n",
        "\n",
        "    emotion_scores = {emotion: 0 for emotion in emotion_lexicon_lemmatized}\n",
        "\n",
        "    for emotion, keywords in emotion_lexicon_lemmatized.items():\n",
        "        for word in tokens + bigrams + trigrams:\n",
        "            if word in keywords:\n",
        "                emotion_scores[emotion] += 1\n",
        "\n",
        "    return emotion_scores\n",
        "\n",
        "def create_feature_matrix(texts):\n",
        "    features = [extract_emotions(text) for text in texts]\n",
        "    return pd.DataFrame(features)\n",
        "\n",
        "X_train = create_feature_matrix(train_df['text'])\n",
        "y_train = train_df[['Joy', 'Fear', 'Anger', 'Sadness', 'Surprise']]\n",
        "\n",
        "X_test = create_feature_matrix(test_df['text'])\n",
        "y_test = test_df[['Joy', 'Fear', 'Anger', 'Sadness', 'Surprise']]"
      ],
      "metadata": {
        "id": "x1C4Il3kSM4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**"
      ],
      "metadata": {
        "id": "Nq04n1H7YtPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Pipeline([\n",
        "    ('clf', OneVsRestClassifier(LogisticRegression(solver='sag',max_iter=1000), n_jobs=1))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'clf__estimator__C': [0.001,0.01, 0.1, 1, 10],\n",
        "    'clf__estimator__solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=20, scoring='f1_macro', verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "y_pred = grid_search.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=['joy', 'fear', 'anger', 'sadness', 'surprise'],zero_division=1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6R5MP8QSU_v",
        "outputId": "a97e1317-5c54-4e84-c1aa-4e5796ce5ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 20 folds for each of 10 candidates, totalling 200 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes**"
      ],
      "metadata": {
        "id": "bQITD1FASnh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Naive Bayes\n",
        "# from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# NB_pipeline = Pipeline([\n",
        "#                 ('clf', OneVsRestClassifier(MultinomialNB(\n",
        "#                     fit_prior=True, class_prior=None))),\n",
        "#             ])\n",
        "\n",
        "\n",
        "# NB_parameters = {\n",
        "#     'clf__estimator__alpha': [0.1, 1.0, 10.0],\n",
        "#     'clf__estimator__fit_prior': [True, False],\n",
        "#     'clf__estimator__class_prior': [None],\n",
        "#     'clf__estimator__force_alpha': [True, False]\n",
        "# }\n",
        "\n",
        "# NB_grid_search = GridSearchCV(NB_pipeline, NB_parameters, cv=20, n_jobs=-1, verbose=1)\n",
        "# NB_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# print(f\"Best parameters found: {NB_grid_search.best_params_}\")\n",
        "\n",
        "# y_pred_NB = NB_grid_search.predict(X_test)\n",
        "# print(\"Classification Report:\")\n",
        "# print(classification_report(y_test, y_pred_NB, target_names=['Joy', 'Fear', 'Anger', 'Sadness', 'Surprise'], zero_division=1))"
      ],
      "metadata": {
        "id": "p0S2E_R-Zhky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Support Vector Machine**"
      ],
      "metadata": {
        "id": "LpHASsdzZ3nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.svm import LinearSVC\n",
        "\n",
        "# SVC_pipeline = Pipeline([\n",
        "#                 ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
        "#             ])\n",
        "\n",
        "# SVC_parameters = {\n",
        "#     'clf__estimator__C': [1.0, 0.1, 0.01],\n",
        "#     'clf__estimator__class_weight': [None, 'balanced']\n",
        "# }\n",
        "\n",
        "# SVC_grid_search = GridSearchCV(SVC_pipeline, SVC_parameters, cv=20, n_jobs=-1, verbose=1)\n",
        "# SVC_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# print(f\"Best parameters found: {SVC_grid_search.best_params_}\")\n",
        "\n",
        "# # Evaluate the best model\n",
        "# y_pred_SVC = SVC_grid_search.predict(X_test)\n",
        "# print(\"Classification Report:\")\n",
        "# print(classification_report(y_test, y_pred_SVC, target_names=['Joy', 'Fear', 'Anger', 'Sadness', 'Surprise'], zero_division=1))"
      ],
      "metadata": {
        "id": "VLhu_apuZrGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest**"
      ],
      "metadata": {
        "id": "pIrVPXvqZ0Tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# RF_pipeline = Pipeline([\n",
        "#                 ('clf', RandomForestClassifier(n_estimators = 100)),])\n",
        "\n",
        "# RF_parameters = {\n",
        "#     'clf__n_estimators': [25, 50, 100, 150],\n",
        "#     'clf__max_features': ['sqrt', 'log2', None],\n",
        "#     'clf__max_depth': [3, 6, 9],\n",
        "#     'clf__max_leaf_nodes': [3, 6, 9]\n",
        "# }\n",
        "\n",
        "# grid_search = GridSearchCV(RF_pipeline, RF_parameters, cv=10, n_jobs=-1, verbose=1)\n",
        "# grid_search.fit(X_train, y_train)\n",
        "\n",
        "# print(f\"Best parameters found: {grid_search.best_params_}\")\n",
        "\n",
        "# y_pred = grid_search.predict(X_test)\n",
        "# print(\"Classification Report:\")\n",
        "# print(classification_report(y_test, y_pred, target_names=['Joy', 'Fear', 'Anger', 'Sadness', 'Surprise'], zero_division=1))"
      ],
      "metadata": {
        "id": "hjxmCj3vZzII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mE1wrzgIS_W2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}